{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Trouver les top 100 pollueurs"
      ],
      "metadata": {
        "id": "HFXcVBvxswGw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Liste des top 100 pollueurs** https://www.theguardian.com/sustainable-business/2017/jul/10/100-fossil-fuel-companies-investors-responsible-71-global-emissions-cdp-study-climate-change?CMP=share_btn_tw"
      ],
      "metadata": {
        "id": "0oGySn2stgx5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Extraction des informations du top 100 sur Wikidata"
      ],
      "metadata": {
        "id": "rTBfODIotfL7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "kV4-DWUhcxwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "pollueurs = {}\n",
        "\n",
        "with open('/Data/Top_pollueurs.csv', newline='') as csvfile:\n",
        "    lecteur_csv = csv.reader(csvfile)\n",
        "    for row in lecteur_csv:\n",
        "        pollueurs[row[0]] = None\n",
        "\n",
        "print(pollueurs)"
      ],
      "metadata": {
        "id": "TzKBuddacu0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "À partir de notre liste de pollueurs, on cherche leur ID Wikidata."
      ],
      "metadata": {
        "id": "_NomEviLdmvU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "def search_entity(name):\n",
        "    url = f\"https://www.wikidata.org/w/api.php?action=wbsearchentities&search={name}&language=en&format=json\"\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        search_results = data.get(\"search\", [])\n",
        "        if search_results:\n",
        "            return search_results[0].get(\"id\")\n",
        "    return None\n",
        "\n",
        "\n",
        "polluters_w_id = {}\n",
        "for entity_name in pollueurs.keys():\n",
        "    entity_id = search_entity(entity_name)\n",
        "    if entity_id:\n",
        "        polluters_w_id[entity_name] = {\"entity_id\": entity_id}\n",
        "\n",
        "print(polluters_w_id)\n"
      ],
      "metadata": {
        "id": "L4vRuoiIt6on"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On cherche ensuite les informations stockées par Wikidata pour chaque pollueurs restants."
      ],
      "metadata": {
        "id": "zkTPnDW5d0DG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def get_entity_info(entity_id):\n",
        "    url = f\"https://www.wikidata.org/w/api.php?action=wbgetentities&ids={entity_id}&format=json&languages=en\"\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        return data.get(\"entities\", {}).get(entity_id, {})\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "with open('/Data/polluters_with_id.json', 'w') as f:\n",
        "    json.dump(polluters_w_id, f)\n",
        "\n",
        "with open('/Data/polluters_with_id.json', 'r') as f:\n",
        "    polluters_w_id = json.load(f)\n",
        "\n",
        "for key, value in polluters_w_id.items():\n",
        "    entity_id = value['entity_id']\n",
        "    entity_info = get_entity_info(entity_id)\n",
        "    value.update(entity_info)\n",
        "\n",
        "with open('/Data/polluters_with_info.json', 'w') as f:\n",
        "    json.dump(polluters_w_id, f)"
      ],
      "metadata": {
        "id": "O8DJ9PyucuCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On termine en nettoyant le fichier pour le rendre plus lisible."
      ],
      "metadata": {
        "id": "nhkZZt6Nejl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/Data/polluters_with_info.json', 'r') as f:\n",
        "    polluters_with_info = json.load(f)\n",
        "\n",
        "for entity_info in polluters_with_info.values():\n",
        "    keys_to_remove = ['pageid', 'ns', 'title', 'lastrevid', 'modified', 'type', 'id', 'labels', 'descriptions', 'aliases', 'claims', 'sitelinks']\n",
        "    for key in keys_to_remove:\n",
        "        entity_info.pop(key, None)\n",
        "\n",
        "for entity, data in polluters_with_info.items():\n",
        "    if 'entity_info' in data:\n",
        "        data['entity_info'].pop('sitelinks', None)\n",
        "\n",
        "with open('/Data/polluters_with_info.json', 'w') as f:\n",
        "    json.dump(polluters_with_info, f)"
      ],
      "metadata": {
        "id": "EP3l5Va0dHjq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Après cela il ne reste plus que 54 pollueurs pour lesquels nous avons des informations complémentaires sur leurs activités."
      ],
      "metadata": {
        "id": "2npTpyGmt79T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Conversion du json en csv pour pouvoir créer un graphe"
      ],
      "metadata": {
        "id": "NBk44nR8uZkG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import csv\n",
        "import requests\n",
        "\n",
        "with open('Data/polluters_with_info.json', 'r') as json_file:\n",
        "    data = json.load(json_file)\n",
        "\n",
        "with open('Data/polluters_with_info.csv', 'w', newline='', encoding='utf-8') as csv_file:\n",
        "    writer = csv.writer(csv_file)\n",
        "\n",
        "    #nous gardons seulement quelques informations : rang, entreprise, id wikidata, pays...\n",
        "    header = [\"Rank\", \"Company\", \"Id\", \"Country\", \"Headquarters\", \"Revenue\", \"Product\", \"Stock Exchange\"]\n",
        "    max_aliases_per_company = 7\n",
        "    for i in range(max_aliases_per_company):\n",
        "        header.append(f\"Alias {i+1}\")\n",
        "    max_sectors_per_company = 4\n",
        "    for i in range(max_sectors_per_company):\n",
        "        header.append(f\"Sector {i+1}\")\n",
        "    max_part_of_per_company = 5\n",
        "    for i in range(max_part_of_per_company):\n",
        "        header.append(f\"Part of {i+1}\")\n",
        "\n",
        "    writer.writerow(header)\n",
        "\n",
        "    rank = 1\n",
        "\n",
        "    #Maintenant, nous cherchons chaque donnée là où elle se situe, en parcourant claims, qui contient toutes les informations sur le pollueur. Les clés nous permettent d'obtenir\n",
        "    #chaque information. Par exemple, la clé pour le pays est P17\n",
        "\n",
        "    claims = entity_info.get(\"claims\", \"\")\n",
        "\n",
        "    for company_name, company_data in data.items():\n",
        "        entity_info = company_data.get(\"entity_info\", {})\n",
        "\n",
        "        id = entity_info.get(\"title\", \"\")\n",
        "\n",
        "        entity_aliases = []\n",
        "        aliases_info = entity_info.get(\"aliases\", {})\n",
        "        for lang, aliases_list in aliases_info.items():\n",
        "            for alias in aliases_list:\n",
        "                entity_aliases.append(alias.get(\"value\", \"\"))\n",
        "        while len(entity_aliases) < max_aliases_per_company:\n",
        "            entity_aliases.append(\"\")\n",
        "\n",
        "\n",
        "\n",
        "        #On cherche le pays du pollueur\n",
        "        state = claims.get(\"P17\", \"\")\n",
        "        if isinstance(state, list):\n",
        "            country = state[0].get(\"mainsnak\", {}).get(\"datavalue\", {}).get(\"value\", {}).get(\"id\", \"\")\n",
        "        else:\n",
        "            country = \"\"\n",
        "\n",
        "        #On cherche le(s) secteur d'activité du pollueur\n",
        "        sector = claims.get(\"P452\", \"\")\n",
        "        sector_list = []\n",
        "        if isinstance(sector, list):\n",
        "            for s in sector:\n",
        "                sector_list.append(s.get(\"mainsnak\", {}).get(\"datavalue\", {}).get(\"value\", {}).get(\"id\", \"\"))\n",
        "        else:\n",
        "            sector_list.append(sector)\n",
        "        while len(sector_list) < max_sectors_per_company:\n",
        "            sector_list.append(\"\")\n",
        "\n",
        "        #On cherche le chiffre d'affaire du pollueur\n",
        "        revenue = claims.get(\"P2139\", \"\")\n",
        "        if isinstance(revenue, list):\n",
        "            revenue = revenue[0].get(\"mainsnak\", {}).get(\"datavalue\", {}).get(\"value\", {}).get(\"amount\", \"\")\n",
        "        else:\n",
        "            revenue = \"\"\n",
        "\n",
        "        #On ajoute la date du revenu\n",
        "        date = claims.get(\"P2139\", \"\")\n",
        "        if isinstance(date, list):\n",
        "            date = date[0].get(\"qualifiers\", {}).get(\"P585\", [{}])[0].get(\"datavalue\", {}).get(\"value\", {}).get(\"time\", \"\")\n",
        "        else:\n",
        "            date = \"\"\n",
        "\n",
        "        revenue = f\"{revenue} ({date})\"\n",
        "\n",
        "        #Lieu du siège\n",
        "        headquarters = claims.get(\"P159\", \"\")\n",
        "        if isinstance(headquarters, list):\n",
        "            headquarters = headquarters[0].get(\"mainsnak\", {}).get(\"datavalue\", {}).get(\"value\", {}).get(\"id\", \"\")\n",
        "        else:\n",
        "            headquarters = \"\"\n",
        "\n",
        "        #Produit, service ou activité principale\n",
        "        product = claims.get(\"P1056\", \"\")\n",
        "        if isinstance(product, list):\n",
        "            product = product[0].get(\"mainsnak\", {}).get(\"datavalue\", {}).get(\"value\", {}).get(\"id\", \"\")\n",
        "        else:\n",
        "            product = \"\"\n",
        "\n",
        "        #Fait partie de ...\n",
        "        part_of = claims.get(\"P361\", \"\")\n",
        "        part_of_list = []\n",
        "        if isinstance(part_of, list):\n",
        "            for p in part_of:\n",
        "                part_of_list.append(p.get(\"mainsnak\", {}).get(\"datavalue\", {}).get(\"value\", {}).get(\"id\", \"\"))\n",
        "        else:\n",
        "            part_of_list.append(part_of)\n",
        "        while len(part_of_list) < max_part_of_per_company:\n",
        "            part_of_list.append(\"\")\n",
        "\n",
        "        stock_exchange = claims.get(\"P414\", \"\")\n",
        "        if isinstance(stock_exchange, list):\n",
        "            stock_exchange = stock_exchange[0].get(\"mainsnak\", {}).get(\"datavalue\", {}).get(\"value\", {}).get(\"id\", \"\")\n",
        "        else:\n",
        "            stock_exchange = \"\"\n",
        "\n",
        "\n",
        "        row = [\n",
        "            rank,\n",
        "            company_name,\n",
        "            id,\n",
        "            country,\n",
        "            headquarters,\n",
        "            revenue,\n",
        "            product,\n",
        "            stock_exchange\n",
        "        ] + entity_aliases + sector_list + part_of_list\n",
        "        writer.writerow(row)\n",
        "\n",
        "        rank += 1"
      ],
      "metadata": {
        "id": "lz9bVVi4ue7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "outputId": "78bd4887-16d7-44f7-c2ba-40a45833d9f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'Data/polluters_with_info.json'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-b2ab282fc899>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Data/polluters_with_info.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Data/polluters_with_info.json'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Un dernière étape pour avoir toutes les informations est la conversion des id en entités par recherche dans wikidata. En effet, nous avons un csv dont les informations ne sont pas complètes.\n",
        "Exemple avec les deux premières lignes :\n",
        "\n",
        "Rank, Company, Id, Country, Headquarters...\n",
        "\n",
        "1, National Iranian Oil Co, Q593733, Q794, Q3616...\n",
        "\n",
        "Il reste alors à chercher les entités dans Wikidata"
      ],
      "metadata": {
        "id": "x407HUHGu-me"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import requests\n",
        "\n",
        "def get_wikidata_entity(entity_id):\n",
        "    url = \"https://www.wikidata.org/w/api.php\"\n",
        "    params = {\n",
        "        \"action\": \"wbgetentities\",\n",
        "        \"ids\": entity_id,\n",
        "        \"format\": \"json\"\n",
        "    }\n",
        "    response = requests.get(url, params=params)\n",
        "    data = response.json()\n",
        "\n",
        "    if \"entities\" in data and entity_id in data[\"entities\"]:\n",
        "        entity = data[\"entities\"][entity_id]\n",
        "        return entity\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def get_entity_name(entity_id):  #nous créons une fonction qui permet de retrouver une entité avec l'id\n",
        "    if entity_id == \"\":\n",
        "        return \"\"\n",
        "    entity = get_wikidata_entity(entity_id)\n",
        "    if isinstance(entity, dict):\n",
        "        labels = entity.get(\"labels\", {})\n",
        "        if \"en\" in labels:\n",
        "            return labels[\"en\"][\"value\"]\n",
        "    return \"\"\n",
        "\n",
        "with open('Data/polluters_with_info_more_info.csv', 'r', encoding='utf-8') as csv_file:\n",
        "    reader = csv.reader(csv_file)\n",
        "    next(reader)\n",
        "\n",
        "    with open('Data/polluters_with_info_more_info_with_names.csv', 'w', newline='', encoding='utf-8') as output_file:\n",
        "        writer = csv.writer(output_file)\n",
        "\n",
        "        writer.writerow(['Rank', 'Company', 'Id', 'Country', 'Headquarters', 'Revenue', 'Product', 'Stock Exchange', 'Alias 1', 'Alias 2', 'Alias 3', 'Alias 4', 'Alias 5', 'Alias 6', 'Alias 7', 'Sector 1', 'Sector 2', 'Sector 3', 'Sector 4', 'Part of 1', 'Part of 2', 'Part of 3', 'Part of 4', 'Part of 5'])\n",
        "\n",
        "        for row in reader:\n",
        "            rank = row[0]\n",
        "            company = row[1]\n",
        "            entity_id = row[2]\n",
        "            country_id = row[3]\n",
        "            headquarters_id = row[4]\n",
        "            revenue = row[5]\n",
        "            product_id = row[6]\n",
        "            stock_exchange_id = row[7]\n",
        "            alias_1 = row[8]\n",
        "            alias_2 = row[9]\n",
        "            alias_3 = row[10]\n",
        "            alias_4 = row[11]\n",
        "            alias_5 = row[12]\n",
        "            alias_6 = row[13]\n",
        "            alias_7 = row[14]\n",
        "            sector_1_id = row[15]\n",
        "            sector_2_id = row[16]\n",
        "            sector_3_id = row[17]\n",
        "            sector_4_id = row[18]\n",
        "            part_of_1_id = row[19]\n",
        "            part_of_2_id = row[20]\n",
        "            part_of_3_id = row[21]\n",
        "            part_of_4_id = row[22]\n",
        "            part_of_5_id = row[23]\n",
        "\n",
        "            #nous cherchons toutes les entités et réécrivons un csv avec les noms\n",
        "            country_name = get_entity_name(country_id)\n",
        "            headquarters_name = get_entity_name(headquarters_id)\n",
        "            product = get_entity_name(product_id)\n",
        "            stock_exchange = get_entity_name(stock_exchange_id)\n",
        "            sector_1_name = get_entity_name(sector_1_id)\n",
        "            sector_2_name = get_entity_name(sector_2_id)\n",
        "            sector_3_name = get_entity_name(sector_3_id)\n",
        "            sector_4_name = get_entity_name(sector_4_id)\n",
        "            part_of_1_name = get_entity_name(part_of_1_id)\n",
        "            part_of_2_name = get_entity_name(part_of_2_id)\n",
        "            part_of_3_name = get_entity_name(part_of_3_id)\n",
        "            part_of_4_name = get_entity_name(part_of_4_id)\n",
        "            part_of_5_name = get_entity_name(part_of_5_id)\n",
        "\n",
        "            writer.writerow([rank, company, entity_id, country_name, headquarters_name, revenue, product, stock_exchange, alias_1, alias_2, alias_3, alias_4, alias_5, alias_6, alias_7, sector_1_name, sector_2_name, sector_3_name, sector_4_name, part_of_1_name, part_of_2_name, part_of_3_name, part_of_4_name, part_of_5_name])"
      ],
      "metadata": {
        "id": "kCKZcFSbvf6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On obtient un csv contenant les 54 pollueurs avec leurs infos : pays, siège, revenu (derniere année où on a l'info), bourse des valeurs (lieu d'échange des valeurs mobilières), alias, secteurs d'activité, groupes dont fait partie l'entreprise"
      ],
      "metadata": {
        "id": "aef-RkqWv1aa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Création de graphe à partir des pollueurs"
      ],
      "metadata": {
        "id": "_fvFgjqGuJbh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Création des noeuds"
      ],
      "metadata": {
        "id": "RuUoK_uRvrlV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous avons créé des noeuds à partir du csv. Pour cela, nous avons écrit un csv nommé nodes.csv qui contient trois colonnes, comme nous l'avons vu en TP : id, label, type."
      ],
      "metadata": {
        "id": "xoYYWsi_AF9v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "with open('Data/polluters_with_info_more_info_with_names.csv', 'r', encoding='utf-8') as input_file:\n",
        "    reader = csv.reader(input_file)\n",
        "    with open('Data/nodes.csv', 'w', encoding='utf-8', newline='') as output_file:\n",
        "        used_labels = []\n",
        "        csv_writer_hq = csv.writer(output_file)\n",
        "        csv_writer_hq.writerow(['Id', 'Label', 'Type'])\n",
        "        next(reader)\n",
        "        for row in reader:\n",
        "            for i in range(1, 24):\n",
        "                #On parcourt toutes les colonnes. En fonction de la colonne dans laquelle on se trouve, le type change. Pour les id, nous avons seulement rajouté un préfixe\n",
        "                #au nom de l'entreprise, ce qui fait bien un clé primaire\n",
        "                if row[i] == '':\n",
        "                    continue\n",
        "                if i == 1:\n",
        "                    id = f'c{row[i]}'\n",
        "                    label = row[i]\n",
        "                    type = 'Company'\n",
        "                elif i == 3 and row[i] != '':\n",
        "                    id = f'country{row[i]}'\n",
        "                    label = row[i]\n",
        "                    type = 'Country'\n",
        "                elif i == 4 and row[i] != '':\n",
        "                    id = f'h{row[i]}'\n",
        "                    label = row[i]\n",
        "                    type = 'Headquarter'\n",
        "                elif i == 5 and row[i] != '':\n",
        "                    id = f'd{row[i]}'\n",
        "                    label = row[i]\n",
        "                    type = 'Revenue'\n",
        "                elif i == 6 and row[i] != '':\n",
        "                    id = f'p{row[i]}'\n",
        "                    label = row[i]\n",
        "                    type = 'Product'\n",
        "                elif i == 7 and row[i] != '':\n",
        "                    id = f's{row[i]}'\n",
        "                    label = row[i]\n",
        "                    type = 'Stock Exchange Sector'\n",
        "                elif 15 <= i <= 18 and row[i] != '':\n",
        "                    id = f'sector{row[i]}'\n",
        "                    label = row[i]\n",
        "                    type = 'Activity Sector'\n",
        "                elif 19 <= i <= 23 and row[i] != '':\n",
        "                    id = f'group{row[i]}'\n",
        "                    label = row[i]\n",
        "                    type = 'Group'\n",
        "                else:\n",
        "                    continue\n",
        "                if label not in used_labels:\n",
        "                    csv_writer_hq.writerow([id, label, type])\n",
        "                    used_labels.append(label)"
      ],
      "metadata": {
        "id": "sgXciYeA1ciP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Créations des arcs"
      ],
      "metadata": {
        "id": "5d9owcrv1fDI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Enfin, nous avons créé des arc entre chaque entreprise et ses différentes caractéristiques. Pour cela, nous avons écrit un csv nommé edges.csv, qui contient trois colonnes, comme vu en TP : Source, Target, Label."
      ],
      "metadata": {
        "id": "lXfDBefLAcGV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "with open('Data/polluters_with_info_more_info_with_names.csv', 'r', encoding='utf-8') as input_file:\n",
        "    reader = csv.reader(input_file)\n",
        "    with open('Data/edges.csv', 'w', encoding='utf-8', newline='') as output_file:\n",
        "        csv_writer_hq = csv.writer(output_file)\n",
        "        csv_writer_hq.writerow(['Source', 'Target', 'Label'])\n",
        "        next(reader)\n",
        "        for row in reader:\n",
        "            for i in range(2, 24):\n",
        "                if row[i] == '': #La source est toujours l'entreprise\n",
        "                    continue\n",
        "                source = f'c{row[1]}'\n",
        "\n",
        "                #Sinon, on cherche target, qu'on retrouve grace aux clés primaires crées, et le label sera différent en fonction de target.\n",
        "\n",
        "                if i == 3 and row[i] != '':\n",
        "                    target = f'country{row[i]}'\n",
        "                    label = 'country'\n",
        "                elif i == 4 and row[i] != '':\n",
        "                    target = f'h{row[i]}'\n",
        "                    label = 'headquarter'\n",
        "                elif i == 5 and row[i] != '':\n",
        "                    target = f'd{row[i]}'\n",
        "                    label = 'revenue'\n",
        "                elif i == 6 and row[i] != '':\n",
        "                    target = f'p{row[i]}'\n",
        "                    label = 'product'\n",
        "                elif i == 7 and row[i] != '':\n",
        "                    target = f's{row[i]}'\n",
        "                    label = 'stock exchange sector'\n",
        "                elif 15 <= i <= 18 and row[i] != '':\n",
        "                    target = f'sector{row[i]}'\n",
        "                    label = 'activity sector'\n",
        "                elif 19 <= i <= 23 and row[i] != '':\n",
        "                    target = f'group{row[i]}'\n",
        "                    label = 'group'\n",
        "                else :\n",
        "                    continue\n",
        "                csv_writer_hq.writerow([source, target, label])"
      ],
      "metadata": {
        "id": "Ipvpw9Y61iR-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "outputId": "c8578f94-af84-48fa-ac6b-26b9b43aca94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'Data/polluters_with_info_more_info_with_names.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-d8eb2f3d5826>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Data/polluters_with_info_more_info_with_names.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0minput_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Data/edges.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moutput_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Data/polluters_with_info_more_info_with_names.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6Y6q3BkTA-Kw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}